{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOromDfsj6e2KjQ7LBi44/c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Taking outputs from ABodyBuilder and MAPT, and embedding associated antibody sequences in Ankh and Ablang.\n","\n","Ablang heavy and light chain embeddings will be concatenated along the row axis, and the combination of features will be used for a regression task to predict hydrophobicity and MAPT score."],"metadata":{"id":"G3gcaM8YDQuA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeN_bD97C2O6"},"outputs":[],"source":["#@title Mount drive and load libraries\n","%%capture\n","!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n","!python -m pip install ankh\n","\n","import os\n","import pandas as pd, numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","\n","path = '/content/drive/MyDrive/msc-project-mbalmf01/msc-project-source-code-files-22-23-mbalmf01/notebooks'\n","os.chdir(path)\n","\n","import torch\n","from plm_manipulation import run_ankh, run_ablang, batch_embed, start_ankh"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230716_human_paired_seqs.csv', dtype={'Run':np.str_}, index_col=0)\n","mapt = pd.read_csv('MAPT/230811_MAPT_scores_abs.csv')\n","\n","linker = 'SGGSTITSYNVYYTKLSSSGT'\n","\n","mapt['scfv'] = mapt['Model Seq L'] + [linker]*mapt.shape[0] + mapt['Model Seq H']\n","\n","df = df[df['scfv'].isin(mapt['scfv'].to_list())]\n","\n","df.to_csv('/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230811_human_paired_seqs_MAPT.csv', index=None)"],"metadata":{"id":"qh6eafckhg0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230811_human_paired_seqs_MAPT.csv', dtype={'Run':np.str_})"],"metadata":{"id":"QqLCZgi27_KZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","model, tokenizer = start_ankh(device=device)\n","\n","prot_col='scfv'\n","seq_id='seq_id'\n","\n","#Ankh embedding is GPU RAM intensive.\n","#Need to split the job up into batches of 5000, which in turn will be split into batches of 100\n","for i in range(0,df.shape[0], 5000):\n","  if i+5000 > df.shape[0]:\n","    tensor_df = batch_embed(df=df.iloc[i:df.shape[0],:], prot_col=prot_col, seq_id=seq_id, batch_size=100, model=model, tokenizer=tokenizer)\n","    tensor_df.to_csv(f'/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230811_human_paired_seqs_MAPT_ankh{i}.csv', index=None)\n","    torch.cuda.empty_cache()\n","    print('completed ankh embedding')\n","  else:\n","    tensor_df = batch_embed(df=df.iloc[i:i+5000,:], prot_col=prot_col, seq_id=seq_id, batch_size=100, model=model, tokenizer=tokenizer)\n","    tensor_df.to_csv(f'/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230811_human_paired_seqs_MAPT_ankh{i}.csv', index=None)\n","    torch.cuda.empty_cache()\n","    print(f'completed {i}')"],"metadata":{"id":"BCKWqUdO8KBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","\n","def disconnect_and_delete_runtime():\n","  \"\"\"Disconnects and deletes the current runtime.\"\"\"\n","  runtime.unassign()\n","\n","disconnect_and_delete_runtime()"],"metadata":{"id":"u9wtm7N4O1xM"},"execution_count":null,"outputs":[]}]}