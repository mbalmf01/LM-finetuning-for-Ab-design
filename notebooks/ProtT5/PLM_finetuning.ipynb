{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOI+u/Ohlj2ZDnGb9Sj2z4X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Supervised fine-tuning on experimental data greatly improves performance of language models compared to using supervised few-shot prediction with embeddings"],"metadata":{"id":"G3gcaM8YDQuA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeN_bD97C2O6"},"outputs":[],"source":["#@title Mount drive and load libraries\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","path = '/content/drive/MyDrive/msc-project-mbalmf01'\n","os.chdir(path)"]},{"cell_type":"code","source":["import transformers\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","import torch\n","from torch.optim import lr_scheduler\n","from torch.utils.tensorboard import SummaryWriter\n","\n","#Load Prot-T5 model\n","model = AutoModelForSequenceClassification.from_pretrained(\"Rostlab/prot-t5\")\n","tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot-t5\")\n","\n","df = pd.read_csv(\"antibody_data.csv\")\n","train_df = df.sample(frac=0.8, random_state=42)\n","val_df = df.drop(train_df.index)\n","\n","#Initialise Prot-T5 model, configure for finetuning.\n","model.config.num_labels = 2\n","\n","model.fit(\n","    x=train_df[\"sequence\"],\n","    y=train_df[\"label\"],\n","    epochs=10,\n","    batch_size=32,\n","    lr=1e-4)\n","\n","model.evaluate(\n","    x=val_df[\"sequence\"],\n","    y=val_df[\"label\"],\n","    batch_size=32)\n"],"metadata":{"id":"FznF3kxbv0_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define callback and learning rate scheduler\n","class LossMonitor(object):\n","    def __init__(self, log_dir):\n","        self.writer = SummaryWriter(log_dir)\n","\n","    def on_epoch_end(self, epoch, loss, val_loss):\n","        self.writer.add_scalar(\"train_loss\", loss, epoch)\n","        self.writer.add_scalar(\"val_loss\", val_loss, epoch)\n","\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","loss_monitor = LossMonitor(\"logs\")\n","\n","#Train model\n","for epoch in range(epochs):\n","    scheduler.step()\n","    train_model(model, dataloader, optimizer, epoch)\n","    val_loss = evaluate_model(model, val_dataloader)\n","    #Log loss to TensorBoard\n","    loss_monitor.on_epoch_end(epoch, loss, val_loss)\n","\n","#Save fine-tuned model\n","model.save_pretrained(\"finetuned_prot-t5\")\n"],"metadata":{"id":"LtXqp0Bbwug5"},"execution_count":null,"outputs":[]}]}