{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyM190GqYByyPd26XtFa8BZM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["#Generating per residue embeddings of heavy and light chains using Ablang\n","\n","Sequences are loaded into the notebook, and then split into batches to avoid saturating the RAM. Amino acid embeddings are generated for each batch, which are then saved to disk as numpy arrays for later use.\n"],"metadata":{"id":"G3gcaM8YDQuA"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"GeN_bD97C2O6","executionInfo":{"status":"ok","timestamp":1692444639553,"user_tz":-60,"elapsed":53408,"user":{"displayName":"Matt Balmforth","userId":"03166723450493542027"}}},"outputs":[],"source":["#@title Mount drive and load libraries\n","%%capture\n","!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n","!python -m pip install ankh\n","\n","import os, gc\n","import pandas as pd, numpy as np, torch\n","from transformers import AutoModel, AutoTokenizer\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","path = '/content/drive/MyDrive/msc-project-mbalmf01/msc-project-source-code-files-22-23-mbalmf01/notebooks'\n","os.chdir(path)\n","\n","from plm_manipulation import chunks, get_aa_embedding"]},{"cell_type":"code","source":["#@title Load in sequences for embedding\n","df = pd.read_csv('/content/drive/MyDrive/msc-project-mbalmf01/all_paired/230813_human_paired_seqs_MAPT_scores.csv', dtype={'Run':np.str_})\n","df.columns"],"metadata":{"id":"QqLCZgi27_KZ","executionInfo":{"status":"ok","timestamp":1692444641613,"user_tz":-60,"elapsed":2066,"user":{"displayName":"Matt Balmforth","userId":"03166723450493542027"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f18aaf99-8a53-4d80-a516-875d99ce780e"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Unnamed: 0', 'Unnamed: 1', 'sequence_id_heavy', 'ANARCI_status_heavy',\n","       'sequence_heavy', 'sequence_alignment_aa_heavy', 'v_call_heavy',\n","       'd_call_heavy', 'j_call_heavy', 'sequence_id_light',\n","       'ANARCI_status_light', 'sequence_light', 'v_call_light', 'j_call_light',\n","       'sequence_alignment_aa_light', 'Run', 'seq_id', 'scfv', 'Acidics Fv',\n","       'Basics Fv', 'Charge pH5 Fv', 'Charge pH5 H', 'Charge pH5 L',\n","       'Charge pH7 Fv', 'Charge pH7 H', 'Charge pH7 L', 'Filename',\n","       'Hydrophobic Surface Fv', 'Hydrophobic Surface H',\n","       'Hydrophobic Surface L', 'MAPSS IgG1 pH5.0', 'MAPSS IgG1 pH7.4',\n","       'MAPSS IgG4P pH5.0', 'MAPSS IgG4P pH7.4', 'Model Seq L'],\n","      dtype='object')"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#@title Load the model and tokenizer from Ablang_heavy\n","%%capture\n","tokenizer = AutoTokenizer.from_pretrained('qilowoq/AbLang_heavy')\n","model = AutoModel.from_pretrained('qilowoq/AbLang_heavy', trust_remote_code=True)"],"metadata":{"id":"JMaiWgMp60s9","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Heavy chain per amino acid embeddings\n","seqs = df['sequence_alignment_aa_heavy'].to_list()\n","max_length = max([len(seq) for seq in seqs])\n","\n","seqs_chunked = chunks(seqs, len(seqs) // 10)\n","\n","for num, seq in enumerate(seqs_chunked):\n","  embeddings = [get_aa_embedding(s, model, tokenizer, max_length) for s in seq]\n","  embeddings = np.concatenate(embeddings)\n","  #save to drive\n","  filepath = f'/content/drive/MyDrive/msc-project-mbalmf01/embeddings/230819_ablang_H_aa_embeddings_{num}.npy'\n","  np.save(filepath, embeddings)\n","  #clear the RAM\n","  del embeddings\n","  gc.collect()"],"metadata":{"id":"Pau0M8mHkOWe","executionInfo":{"status":"ok","timestamp":1692448967125,"user_tz":-60,"elapsed":4132170,"user":{"displayName":"Matt Balmforth","userId":"03166723450493542027"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["file_size = os.path.getsize('/content/drive/MyDrive/msc-project-mbalmf01/embeddings/230819_ablang_H_aa_embeddings_0.npy')\n","print(f'file size of each embedding numpy array is {round(file_size/1073741824, 3)}GB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKOJyB6V62GX","executionInfo":{"status":"ok","timestamp":1692449613236,"user_tz":-60,"elapsed":273,"user":{"displayName":"Matt Balmforth","userId":"03166723450493542027"}},"outputId":"0b1f2218-bfb9-43e0-db90-e0535961c29f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["file size of each embedding numpy array is 2.682GB\n"]}]},{"cell_type":"code","source":["#@title Repeat for light chain embeddings\n","tokenizer = AutoTokenizer.from_pretrained('qilowoq/AbLang_light')\n","model = AutoModel.from_pretrained('qilowoq/AbLang_light', trust_remote_code=True)\n","\n","seqs = df['sequence_alignment_aa_light'].to_list()\n","max_length = max([len(seq) for seq in seqs])\n","\n","for num, seq in enumerate(seqs_chunked):\n","  embeddings = [get_aa_embedding(s, model, tokenizer, max_length) for s in seq]\n","  embeddings = np.concatenate(embeddings)\n","  filepath = f'/content/drive/MyDrive/msc-project-mbalmf01/embeddings/230819_ablang_L_aa_embeddings_{num}.npy'\n","  np.save(filepath, embeddings)\n","  del embeddings\n","  gc.collect()"],"metadata":{"id":"q11HgRPNcWrp"},"execution_count":null,"outputs":[]}]}